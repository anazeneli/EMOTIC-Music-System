{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras_vggface.vggface import VGGFace\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.utils.data_utils import get_file\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating body boxes\n",
      "Creating body boxes\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary of annotations structures\n",
    "# hashed by file name\n",
    "# length of value is number of persons annotated\n",
    "# [['person'],['gender'],['annotations_categories'],['annotations_continuous'],['gender'],['age']]\n",
    "def create_emotic_dict(data, test= False ): \n",
    "    # set variables based on testing/training dataset \n",
    "    # using combined avg on test dataset because \n",
    "    # multiple annotators used \n",
    "    if test: \n",
    "        categories = 'combined_categories'\n",
    "        continuous  = 'combined_continuous'\n",
    "    else: \n",
    "        categories = 'annotations_categories'\n",
    "        continuous  = 'annotations_continuous'\n",
    "    \n",
    "    \n",
    "#     test_image = 'framesdb/images/frame_ghkq7yp0itqz0kn7.jpg'\n",
    "    emotic_annotations = {} \n",
    "    \n",
    "    print (\"Creating body boxes\")\n",
    "    for i in range(len(data['filename'][0])): \n",
    "        file = data['filename'][0][i][0]\n",
    "        # if more than one person is the focus of the image \n",
    "        for j in range(len(data['person'][0][i][0])):      \n",
    "            file_path = str(data['folder'][0][i][0]) + \"/\" + file\n",
    "            \n",
    "            if file_path in emotic_annotations: \n",
    "                emotic_annotations[file_path] = np.append( emotic_annotations[file_path], \n",
    "                                                # (x1, y1, x2, y2)\n",
    "                                                [[tuple([int(i) for i in np.array(data['person'][0][i][0][j]['body_bbox'][0]).tolist()]),\n",
    "                                                  np.array(np.array(np.array(data['person'][0][i][0][j][categories]).tolist()).tolist()).flatten().tolist(),      \n",
    "                                                  np.array(np.array(data['person'][0][i][0][j][continuous]).tolist()).flatten().tolist(),\n",
    "                                                  np.array(data['person'][0][i][0][j]['gender']).tolist(),\n",
    "                                                  np.array(data['person'][0][i][0][j]['age']).tolist()]], \n",
    "                                                  axis = 0 )\n",
    "\n",
    "            else: \n",
    "                emotic_annotations[file_path]= np.array([[tuple([int(i) for i in np.array(data['person'][0][i][0][j]['body_bbox'][0]).tolist()]),\n",
    "                                               np.array(np.array(np.array(data['person'][0][i][0][j][categories]).tolist()).tolist()).flatten().tolist(),                             \n",
    "                                               np.array(np.array(data['person'][0][i][0][j][continuous]).tolist()).flatten().tolist(),\n",
    "                                               np.array(data['person'][0][i][0][j]['gender']).tolist(),\n",
    "                                               np.array(data['person'][0][i][0][j]['age']).tolist()]])\n",
    "    \n",
    "    return emotic_annotations\n",
    "\n",
    "emotic = scipy.io.loadmat('EMOTIC/annotations/Annotations.mat')\n",
    "train = create_emotic_dict(emotic['train'])\n",
    "test = create_emotic_dict(emotic['test'], True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " class PersonFeatures():\n",
    "    def __init__(self, train, test):\n",
    "        print (\"LETS DO THIS \")\n",
    "        # pull out body boxes\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        \n",
    "        print (\"XTRAIN\")\n",
    "        X_train = self.extract_bodies(self.train)\n",
    "        print (\"XTEST\")\n",
    "        X_test = self.extract_bodies(self.test, True)\n",
    "\n",
    "        # Compute a PCA \n",
    "        n_components = 100\n",
    "        print (\"PCA\")\n",
    "        pca = PCA(n_components=n_components, whiten=True).fit(X_train)\n",
    "\n",
    "        # apply PCA transformation\n",
    "        X_train_pca = pca.transform(X_train)\n",
    "        X_test_pca = pca.transform(X_test)          \n",
    " \n",
    "        self.train_feats = self.extract_features(X_train_pca)\n",
    "        self.test_feats = self.extract_features(X_test_pca)\n",
    "\n",
    "\n",
    "    def extract_bodies(self, images, test = False):\n",
    "        print (\"BODIES\")\n",
    "\n",
    "        dir_path = \"EMOTIC/emotic/\"\n",
    "        out_path = \"EMOTIC/emotic/cropped/train/\"\n",
    "        if test: \n",
    "            out_path = \"EMOTIC/emotic/cropped/test/\"\n",
    "\n",
    "#         test_image = 'framesdb/images/frame_ghkq7yp0itqz0kn7.jpg'\n",
    "        # crop images\n",
    "        cropped_images = []\n",
    "        # load in training images body boxes\n",
    "        for k, v in tqdm(images.items()):\n",
    "            for b in v[:, 0]:\n",
    "                # read image for cropping\n",
    "                img = cv2.imread(dir_path+k)\n",
    "    #             img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#                 cv2.imshow(\"image\", img)\n",
    "                cropped_img = img[ b[1]:b[3], b[0]:b[2] ]\n",
    "    #                 cv2.imshow(\"cropped\", cropped_img)\n",
    "#                     cv2.waitKey(0)\n",
    "    #                 cv2.imwrite(out_path + k, cropped_img)\n",
    "\n",
    "                cropped_images.append(cropped_img)\n",
    "    \n",
    "        return cropped_images\n",
    "\n",
    "        \n",
    "    def extract_features(self, images):\n",
    "        print (\"FEATURES\")\n",
    "        \n",
    "        face_model = VGGFace(model='resnet50', include_top=False)\n",
    "\n",
    "        # list of pre-processed images \n",
    "        imgs = [] \n",
    "        for i in images: \n",
    "            # load image setting the image size to 224 x 224\n",
    "            target_size = (224, 224)\n",
    "            img = cv2.imread(i,1)\n",
    "            img = cv2.resize(img, target_size)\n",
    "#             cv2.imshow('image',img)\n",
    "#             cv2.waitKey(0)\n",
    "\n",
    "            # convert image to numpy array\n",
    "            x = image.img_to_array(img)\n",
    "            # the image is now in an array of shape (3, 224, 224) \n",
    "            # need to expand it to (1, 3, 224, 224) as preprocess takes in a list\n",
    "            x = np.expand_dims(x, axis=0)\n",
    "            # ensure proper input format for model \n",
    "            x = preprocess_input(x)\n",
    "            \n",
    "            # extract the features\n",
    "            imgs.append(x) \n",
    "            \n",
    "        imgs = np.concatenate(imgs, axis=0)\n",
    "        img_feats = face_model.predict(imgs)\n",
    "        \n",
    "        return img_feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LETS DO THIS \n",
      "XTRAIN\n",
      "BODIES\n",
      "XTEST\n",
      "BODIES\n"
     ]
    }
   ],
   "source": [
    "p = PersonFeatures(train, test)\n",
    "\n",
    "print(p.train_feats)\n",
    "print(p.test_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
