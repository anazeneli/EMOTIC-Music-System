{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from person_features import PersonFeatures\n",
    "from global_features import GlobalFeatures\n",
    "from fusion_module import FusionModule\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from pygame import mixer\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import cv2\n",
    "\n",
    "test_image = \"0n5sa6o2zfrg3shtoo.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of annotations structures\n",
    "# hashed by file name\n",
    "# length of value is number of persons annotated\n",
    "# [['person'],['gender'],['annotations_categories'],['annotations_continuous'],['gender'],['age']]\n",
    "emotic_annotations = {}\n",
    "def create_emotic_dict(data, test= False ): \n",
    "    # set variables based on testing/training dataset \n",
    "    # using combined avg on test dataset because \n",
    "    # multiple annotators used \n",
    "    if test: \n",
    "        categories = 'combined_categories'\n",
    "        continuous  = 'combined_continuous'\n",
    "    else: \n",
    "        categories = 'annotations_categories'\n",
    "        continuous  = 'annotations_continuous'\n",
    "    \n",
    "    \n",
    "    test_image = 'framesdb/images/frame_ghkq7yp0itqz0kn7.jpg'\n",
    "    emotic_annotations = {} \n",
    "    \n",
    "    print (\"Creating body boxes\")\n",
    "    for i in range(len(data['filename'][0])): \n",
    "        file = data['filename'][0][i][0]\n",
    "        # if more than one person is the focus of the image \n",
    "        for j in range(len(data['person'][0][i][0])):      \n",
    "            file_path = str(data['folder'][0][i][0]) + \"/\" + file\n",
    "            \n",
    "            if file_path in emotic_annotations: \n",
    "                emotic_annotations[file_path] = np.append( emotic_annotations[file_path], \n",
    "                                                [[tuple(np.array(data['person'][0][i][0][j]['body_bbox'][0]).tolist()),\n",
    "                                                  np.array(np.array(np.array(data['person'][0][i][0][j][categories]).tolist()).tolist()).flatten().tolist(),      \n",
    "                                                  np.array(np.array(data['person'][0][i][0][j][continuous]).tolist()).flatten().tolist(),\n",
    "                                                  np.array(data['person'][0][i][0][j]['gender']).tolist(),\n",
    "                                                  np.array(data['person'][0][i][0][j]['age']).tolist()]], \n",
    "                                                  axis = 0 )\n",
    "\n",
    "            else: \n",
    "                emotic_annotations[file_path]= np.array([[tuple(np.array(data['person'][0][i][0][j]['body_bbox'][0]).tolist()),\n",
    "                                               np.array(np.array(np.array(data['person'][0][i][0][j][categories]).tolist()).tolist()).flatten().tolist(),                             \n",
    "                                               np.array(np.array(data['person'][0][i][0][j][continuous]).tolist()).flatten().tolist(),\n",
    "                                               np.array(data['person'][0][i][0][j]['gender']).tolist(),\n",
    "                                               np.array(data['person'][0][i][0][j]['age']).tolist()]])\n",
    "    \n",
    "    return emotic_annotations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in emotic annotations\n",
      "Creating body boxes\n",
      "Creating body boxes\n",
      "Creating body boxes\n"
     ]
    }
   ],
   "source": [
    "# load in emotic annotations\n",
    "# Annotations = {train, val, test} \n",
    "# filename, folder, image_size, original_database, person \n",
    "# person: body_bbox, annotations_categories, annotations_continuous, gender, age\n",
    "print (\"Reading in emotic annotations\")\n",
    "emotic = scipy.io.loadmat('EMOTIC/annotations/Annotations.mat')\n",
    "\n",
    "# dictionary for training and test annotations \n",
    "# [['person'],['gender'],['annotations_categories'],['annotations_continuous'],['gender'],['age']]\n",
    "train_dict = create_emotic_dict(emotic['train'])\n",
    "val_dict   = create_emotic_dict(emotic['val'], True)\n",
    "test_dict  = create_emotic_dict(emotic['test'], True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mscoco/images/COCO_train2014_000000001424.jpg', 'mscoco/images/COCO_train2014_000000416690.jpg', 'mscoco/images/COCO_train2014_000000416651.jpg', 'mscoco/images/COCO_train2014_000000073568.jpg', 'mscoco/images/COCO_train2014_000000020651.jpg', 'mscoco/images/COCO_train2014_000000076992.jpg', 'mscoco/images/COCO_train2014_000000572179.jpg', 'mscoco/images/COCO_train2014_000000569401.jpg']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "test_dir = 'prototype_imgs/'\n",
    "\n",
    "\n",
    "test_files = [\"mscoco/images/\"+i for i in os.listdir(test_dir) if 'jpg' in i]\n",
    "print(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "mscoco/images/COCO_train2014_000000020651.jpg (9.5, 7.0)\n",
      "[(357, 5, 594, 359) list(['Anticipation', 'Excitement', 'Happiness'])\n",
      " list([10, 7, 6]) list(['Female']) list(['Teenager'])]\n",
      "2\n",
      "mscoco/images/COCO_train2014_000000416690.jpg (6.0, 8.0)\n",
      "[(175, 109, 420, 472) list(['Excitement', 'Happiness', 'Surprise'])\n",
      " list([6, 8, 8]) list(['Male']) list(['Kid'])]\n",
      "3\n",
      "mscoco/images/COCO_train2014_000000001424.jpg (5.0, 4.0)\n",
      "[(36, 191, 195, 370) list(['Disconnection', 'Doubt/Confusion'])\n",
      " list([5, 4, 2]) list(['Female']) list(['Kid'])]\n",
      "4\n",
      "mscoco/images/COCO_train2014_000000416651.jpg (5.0, 5.0)\n",
      "[(271, 63, 401, 516) list(['Engagement']) list([5, 5, 7]) list(['Male'])\n",
      " list(['Adult'])]\n",
      "5\n",
      "mscoco/images/COCO_train2014_000000076992.jpg (8.0, 7.0)\n",
      "[(19, 55, 328, 327) list(['Anticipation', 'Engagement']) list([8, 7, 7])\n",
      " list(['Male']) list(['Adult'])]\n",
      "6\n",
      "mscoco/images/COCO_train2014_000000572179.jpg (6.0, 5.5)\n",
      "[(297, 27, 548, 308) list(['Anticipation', 'Engagement']) list([6, 7, 7])\n",
      " list(['Male']) list(['Adult'])]\n",
      "7\n",
      "mscoco/images/COCO_train2014_000000073568.jpg (5.0, 4.0)\n",
      "[(195, 38, 397, 327) list(['Engagement']) list([5, 4, 7]) list(['Female'])\n",
      " list(['Adult'])]\n",
      "8\n",
      "mscoco/images/COCO_train2014_000000569401.jpg (6.0, 4.0)\n",
      "[(52, 77, 517, 418) list(['Confidence', 'Engagement']) list([6, 4, 6])\n",
      " list(['Male']) list(['Adult'])]\n"
     ]
    }
   ],
   "source": [
    "photo_avgs = {} \n",
    "for k,v in train_dict.items(): \n",
    "    if k in test_files:\n",
    "        mean_v = 0 \n",
    "        mean_a = 0 \n",
    "\n",
    "        for i in v: \n",
    "            mean_v += i[2][0]\n",
    "            mean_a +=i[2][1]\n",
    "            \n",
    "        photo_avgs[k] = (mean_v/len(v), mean_a/len(v))\n",
    "   \n",
    "count = 0\n",
    "for k,v in photo_avgs.items(): \n",
    "    count += 1\n",
    "\n",
    "    print (count)\n",
    "    print(k,v)\n",
    "    print (train_dict[k][0])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['Peace', 'Affection', 'Esteem', 'Anticipation', 'Engagement','Confidence','Happiness','Pleasure','Excitement','Surprise','Sympathy','Doubt/Confusion','Disconnection','Fatigue','Embarrassment','Yearning','Disapproval','Aversion','Annoyance','Anger','Sensitivity','Sadness','Disquietment','Fear','Pain','Suffering']\n",
    "basic = [emotions[6], emotions[9], emotions[17], emotions[19], emotions[21], emotions[23]]\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "           weights='distance')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import neighbors \n",
    "\n",
    "songs_csv = 'DEAM/annotations/annotations_avg_per_song/song_level/26_category_emotions.csv'\n",
    "songs_df = pd.read_csv(songs_csv)\n",
    "songs_df.sort_values(by=['emotion assignment','song_id'], inplace=True, ascending=True)\n",
    "songs_df = songs_df.reset_index()\n",
    "\n",
    "X = songs_df[['valence_mean', 'arousal_mean']]\n",
    "y = songs_df['emotion assignment']\n",
    "\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors=3, weights='distance')  \n",
    "clf.fit(X,y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.neighbors import KDTree\n",
    "\n",
    "# X = np.array(songs_df[['valence_mean', 'arousal_mean']])  # 3 points in 2 dimensions\n",
    "# tree = KDTree(X)\n",
    "# dist, ind = tree.query([[9.5, 7.0]], k=5)\n",
    "# print(ind[0][0])  # indices of 2 closest neighbors\n",
    "# # print(dist)  # distances to 2 closest neighbors\n",
    "# x = songs_df['emotion assignment'].iloc[ind[0][0]]\n",
    "\n",
    "# print (emotions[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 23, 25, 17, 18, 16, 25, 4]\n"
     ]
    }
   ],
   "source": [
    "videos_emot = [] \n",
    "for k,v in photo_avgs.items(): \n",
    "#     print(k)\n",
    "    emot= clf.predict([v])[0]\n",
    "    neigh=clf.kneighbors(X, return_distance=False) \n",
    "    videos_emot.append(emot)\n",
    "#     print(emot)\n",
    "#     print(emotions[emot])\n",
    "     \n",
    "print (videos_emot)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# songs_df.sort_values(by=['emotion assignment','song_id'], inplace=True, ascending=True)\n",
    "# songs_df = songs_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, k in enumerate(photo_avgs.keys()): \n",
    "    # play emotion-tagged music \n",
    "    mixer.init()\n",
    "    music_dir = 'DEAM/MEMD_audio/'\n",
    "    # select music by valence and arousal scores! \n",
    "    emot = videos_emot[i]\n",
    "    df = songs_df['song_id'].loc[songs_df['emotion assignment'] == emot]\n",
    "    song_id= df.sample(n=1).iloc[0]\n",
    "#     print (k)\n",
    "#     img = cv2.imread(k) \n",
    "#     cv2.imshow(img, 1)\n",
    "#     cv2.waitKey(0)\n",
    "    \n",
    "    mixer.music.load(music_dir + str(song_id) + \".mp3\")\n",
    "    mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import eyed3\n",
    "\n",
    "# audiofile = eyed3.load('example.mp3')\n",
    "# if (audiofile.tag == None):\n",
    "#     audiofile.initTag()\n",
    "\n",
    "# audiofile.tag.images.set(3, open('cover.jpg','rb').read(), 'image/jpeg')\n",
    "\n",
    "# audiofile.tag.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing import image\n",
    "# from keras.applications.vgg16 import VGG16\n",
    "# from keras.applications.vgg16 import preprocess_input\n",
    "# from keras.utils.data_utils import get_file\n",
    "# train_filelist = np.array(train_dict.keys()).tolist()\n",
    "# dir_path = \"EMOTIC/emotic/\"\n",
    "\n",
    "# proccesed_ims = [] \n",
    "# for i in tqdm(train_filelist): \n",
    "#     img_path = dir_path + i \n",
    "#     img = image.load_img(img_path, target_size=(224, 224))\n",
    "#     # convert image to numpy array\n",
    "#     x = image.img_to_array(img)\n",
    "#     # the image is now in an array of shape (3, 224, 224) \n",
    "#     # need to expand it to (1, 3, 224, 224) as preprocess takes in a list\n",
    "#     x = np.expand_dims(x, axis=0)\n",
    "#     x = preprocess_input(x)\n",
    "#     proccesed_ims.append(x)\n",
    "    \n",
    "# x = np.array(proccesed_ims)\n",
    "# x.dump(\"train_imgs.npy\")\n",
    "# print (\"DUMPED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proccesed_ims_test = [] \n",
    "# test_filelist = np.array(test_dict.keys()).tolist()\n",
    "\n",
    "# for i in tqdm(test_filelist): \n",
    "#     img_path = dir_path + i \n",
    "#     img = image.load_img(img_path, target_size=(224, 224))\n",
    "#     # convert image to numpy array\n",
    "#     x = image.img_to_array(img)\n",
    "#     # the image is now in an array of shape (3, 224, 224) \n",
    "#     # need to expand it to (1, 3, 224, 224) as preprocess takes in a list\n",
    "#     x = np.expand_dims(x, axis=0)\n",
    "#     x = preprocess_input(x)\n",
    "#     proccesed_ims_test.append(x)\n",
    "    \n",
    "# x = np.array(proccesed_ims_test)\n",
    "# x.dump(\"test_imgs.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for k,v in emotic_annotations.items(): \n",
    "# #     print(k, v)\n",
    "    \n",
    "# # pass bounding box parameters to person to handle\n",
    "# person_features = PersonFeatures(train_dict, test_dict)\n",
    "\n",
    "# global_features = GlobalFeatures(train_dict['train']['filename'])\n",
    "# print (global_features)\n",
    "# # fusion = GlobalFeatures(annotations['train']['filename'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
